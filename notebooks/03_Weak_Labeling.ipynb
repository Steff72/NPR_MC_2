{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Weak Labeling\n",
                "\n",
                "This notebook generates weak labels for the unlabeled data using k-Nearest Neighbors on sentence embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
                "\n",
                "from data_loader import load_and_split_data\n",
                "from weak_labeling import WeakLabeler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "splits = load_and_split_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Weak Labeler\n",
                "\n",
                "We use `all-mpnet-base-v2` for generating embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labeler = WeakLabeler(model_name=\"all-mpnet-base-v2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate and Evaluate Weak Labels\n",
                "\n",
                "For each split size, we train a k-NN on the labeled set and predict labels for the unlabeled set. We then compare these weak labels with the true labels (which we have access to for evaluation purposes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_sizes = [100, 250, 500, 1000]\n",
                "results = []\n",
                "\n",
                "for size in train_sizes:\n",
                "    print(f\"\\n=== Weak Labeling for {size} labeled samples ===\")\n",
                "    train_df = splits[f'train_{size}']\n",
                "    unlabeled_df = splits[f'unlabeled_{size}']\n",
                "    \n",
                "    # Train k-NN\n",
                "    knn = labeler.train_knn(train_df, n_neighbors=5)\n",
                "    \n",
                "    # Predict\n",
                "    weak_labeled_df = labeler.predict(knn, unlabeled_df)\n",
                "    \n",
                "    # Evaluate (comparing weak labels to true labels hidden in unlabeled_df)\n",
                "    # Note: unlabeled_df still has the 'label' column with true labels\n",
                "    true_labels = unlabeled_df['label']\n",
                "    predicted_labels = weak_labeled_df['label']\n",
                "    \n",
                "    acc = accuracy_score(true_labels, predicted_labels)\n",
                "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
                "    \n",
                "    print(f\"Weak Label Quality (Size {size}): Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
                "    \n",
                "    results.append({\n",
                "        'train_size': size,\n",
                "        'weak_accuracy': acc,\n",
                "        'weak_f1': f1\n",
                "    })\n",
                "    \n",
                "    # Save weak labeled data for next step (optional, or just re-generate)\n",
                "    # weak_labeled_df.to_csv(f\"../data/weak_labeled_{size}.csv\", index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plot Weak Label Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(results_df['train_size'], results_df['weak_f1'], marker='o', label='Weak Label F1')\n",
                "plt.plot(results_df['train_size'], results_df['weak_accuracy'], marker='s', label='Weak Label Accuracy')\n",
                "plt.title('Quality of Weak Labels vs Seed Training Size')\n",
                "plt.xlabel('Number of Labeled Samples')\n",
                "plt.ylabel('Score')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}